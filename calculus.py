# -*- coding: utf-8 -*-
"""Calculus.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11o6qKpH-J_YWDNY79Gd05Z_R6nCWDLHu
"""



"""Basics Of Calculus

Calculus- Is a branch of Math that deals with continous change, involving derrivatives and the functions change over time and for optimization of machine learning algorithms.

Differentiation/derrivatives

- The derrivative of a function represents the rate of change or the slope of that function at a point.
- To work with derrivatives and integrals in the python programming language, we use the sympy language
"""

import sympy as sp

# define the variable
x= sp.symbols('x')

#define the function
f= x**2+3*x+2

# calculate the derrivative
derivative= sp.diff(f,x)

print(f"The derrivative of the function {f} is: {derivative}")

f= x**3+3*x+2
#calculate the derivative
derivative= sp.diff(f,x)
print(f"The derrivative of the function {f} is: {derivative}")

f= x**5+3*x**3+2*x+1
#calculate the derivative
derivative= sp.diff(f,x)
print(f"The derrivative of the function {f} is: {derivative}")

"""Intergration/Integrals- The integral of a function is represents the area under a curve, summing up all the infinitely small areas


"""

x= sp.symbols('x')
f= x**2+3*x+2
integral= sp.integrate(f,x)
print(f"The integral of the function {f} is: {integral}+C")

f= 6*x**2-5
integral= sp.integrate(f,x)
print(f"The integral of the function {f} is: {integral}+C")

"""Areas of Application

- Back propagation in neural networks- is an optimization technique that reduces the loss function and increases accuracy in neural network machine learning

- Gradient descent algorithm- is an optimization technique that changes the parameters to better suit the model.

- Support vector/ Support Vector Classifiers- are supervised ML models that draw a hyperplane to maximize the distance between the plane and the nearest datapoints on either class
"""